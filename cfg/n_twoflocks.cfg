[DEFAULT]

alg = dagger

# learning parameters
batch_size = 20
buffer_size = 5000
updates_per_step = 200
seed = 12
actor_lr = 5e-5

n_train_episodes = 400
beta_coeff = 0.993
test_interval = 40
n_test_episodes = 5

# architecture parameters
k = 2
hidden_size = 32
gamma = 0.99
tau = 0.5

# env parameters
env = FlockingTwoFlocks-v0
v_max = 3.0
comm_radius = 1.0
n_agents = 100
n_actions = 2
n_states = 6
debug = False
dt = 0.01


header = k, n_agents, reward

[4, 50]
k = 4
n_agents = 50

[4, 100]
k = 4
n_agents = 100

[4, 150]
k = 4
n_agents = 150

;[4, 200]
;k = 4
;n_agents = 200
;
;[4, 250]
;k = 4
;n_agents = 250


[1, 50]
k = 1
n_agents = 50

[1, 100]
k = 1
n_agents = 100

[1, 150]
k = 1
n_agents = 150

;[1, 200]
;k = 1
;n_agents = 200
;
;[1, 250]
;k = 1
;n_agents = 250


[2, 50]
k = 2
n_agents = 50

[2, 100]
k = 2
n_agents = 100

[2, 150]
k = 2
n_agents = 150

;[2, 200]
;k = 2
;n_agents = 200
;
;[2, 250]
;k = 2
;n_agents = 250



[3, 50]
k = 3
n_agents = 50

[3, 100]
k = 3
n_agents = 100

[3, 150]
k = 3
n_agents = 150

[3, 200]
k = 3
n_agents = 200

[3, 250]
k = 3
n_agents = 250







