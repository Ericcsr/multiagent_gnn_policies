[DEFAULT]

alg = dagger

# learning parameters
batch_size = 20
buffer_size = 10000
updates_per_step = 200
seed = 11
actor_lr = 5e-5

n_train_episodes = 800
beta_coeff = 0.993
test_interval = 40
n_test_episodes = 20

# architecture parameters
k = 2
hidden_size = 32
gamma = 0.99
tau = 0.5

# env parameters
env = FlockingRelative-v0
v_max = 3.0
comm_radius = 1.0
n_agents = 100
n_actions = 2
n_states = 6
debug = False
dt = 0.01


header = k, dt, reward

[1, 0.1]
k = 1
dt = 0.1

[1, 0.075]
k = 1
dt = 0.075

[1, 0.05]
k = 1
dt = 0.05

[1, 0.025]
k = 1
dt = 0.025

[1, 0.01]
k = 1
dt = 0.01

[1, 0.0075]
k = 1
dt = 0.0075

[2, 0.1]
k = 2
dt = 0.1

[2, 0.075]
k = 2
dt = 0.075

[2, 0.05]
k = 2
dt = 0.05

[2, 0.025]
k = 2
dt = 0.025

[2, 0.01]
k = 2
dt = 0.01

[2, 0.0075]
k = 2
dt = 0.0075

[3, 0.1]
k = 3
dt = 0.1

[3, 0.075]
k = 3
dt = 0.075

[3, 0.05]
k = 3
dt = 0.05

[3, 0.025]
k = 3
dt = 0.025

[3, 0.01]
k = 3
dt = 0.01

[3, 0.0075]
k = 3
dt = 0.0075

[4, 0.1]
k = 4
dt = 0.1

[4, 0.075]
k = 4
dt = 0.075

[4, 0.05]
k = 4
dt = 0.05

[4, 0.025]
k = 4
dt = 0.025

[4, 0.01]
k = 4
dt = 0.01

[4, 0.0075]
k = 4
dt = 0.0075

